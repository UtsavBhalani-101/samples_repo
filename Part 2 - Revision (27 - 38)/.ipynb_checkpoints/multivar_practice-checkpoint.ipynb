{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "265f2c7f-2240-449e-898f-8e4dd98086a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset head:\n",
      "   id  age    city  has_subscription  sensor_reading preferred_device  \\\n",
      "0   1   56  Berlin              True          110.46           Laptop   \n",
      "1   2   69   Paris             False           88.53           Tablet   \n",
      "2   3   46   Paris              True             NaN           Tablet   \n",
      "3   4   32   Paris              True          142.85            Phone   \n",
      "4   5   60  London             False          134.55          Desktop   \n",
      "\n",
      "   reported_income satisfaction_level  \n",
      "0          50307.0            Neutral  \n",
      "1              NaN        Unsatisfied  \n",
      "2          17233.0     Very Satisfied  \n",
      "3          40273.0            Neutral  \n",
      "4              NaN            Neutral  \n",
      "\n",
      "Missing values per column:\n",
      "id                      0\n",
      "age                     0\n",
      "city                    0\n",
      "has_subscription        0\n",
      "sensor_reading         75\n",
      "preferred_device       50\n",
      "reported_income       112\n",
      "satisfaction_level     57\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "id                      int64\n",
      "age                     int32\n",
      "city                   object\n",
      "has_subscription         bool\n",
      "sensor_reading        float64\n",
      "preferred_device       object\n",
      "reported_income       float64\n",
      "satisfaction_level     object\n",
      "dtype: object\n",
      "\n",
      "--- Verifying NMAR ---\n",
      "\n",
      "Reported Income (Numerical NMAR):\n",
      "Distribution of non-missing reported_income:\n",
      "count      388.000000\n",
      "mean     42472.641753\n",
      "std      12987.465528\n",
      "min      10000.000000\n",
      "25%      32846.250000\n",
      "50%      42779.500000\n",
      "75%      51292.750000\n",
      "max      94429.000000\n",
      "Name: reported_income, dtype: float64\n",
      "\n",
      "Satisfaction Level (Categorical NMAR):\n",
      "Value counts of non-missing satisfaction_level:\n",
      "satisfaction_level\n",
      "Satisfied           0.334086\n",
      "Neutral             0.311512\n",
      "Unsatisfied         0.171558\n",
      "Very Satisfied      0.153499\n",
      "Very Unsatisfied    0.029345\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Original (true) distribution of satisfaction_level (for comparison):\n",
      "Satisfied           0.310\n",
      "Neutral             0.290\n",
      "Unsatisfied         0.160\n",
      "Very Satisfied      0.142\n",
      "Very Unsatisfied    0.098\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "SUCCESS: Not all columns have missing values.\n",
      "\n",
      "Number of columns with missing values: 4\n",
      "SUCCESS: At least 4 columns have missing values.\n",
      "\n",
      "Summary of designed missingness:\n",
      "- sensor_reading (Numerical MCAR): Missingness should be random.\n",
      "- preferred_device (Categorical MCAR): Missingness should be random.\n",
      "- reported_income (Numerical NMAR): Missingness depends on its own unobserved value (higher for very low/high incomes).\n",
      "- satisfaction_level (Categorical NMAR): Missingness depends on its own unobserved value (higher for 'Very Unsatisfied').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of rows in the dataset\n",
    "n_rows = 500\n",
    "\n",
    "# --- 1. Base Columns (No Missing Data) ---\n",
    "data = {\n",
    "    'id': range(1, n_rows + 1),\n",
    "    'age': np.random.randint(18, 70, size=n_rows),\n",
    "    'city': np.random.choice(['New York', 'London', 'Paris', 'Tokyo', 'Berlin'], size=n_rows, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "    'has_subscription': np.random.choice([True, False], size=n_rows, p=[0.6, 0.4])\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- 2. Numerical Column with MCAR: `sensor_reading` ---\n",
    "# Generate some sensor data\n",
    "df['sensor_reading'] = np.random.normal(loc=100, scale=20, size=n_rows).round(2)\n",
    "# Introduce MCAR missingness (approx 15%)\n",
    "mcar_indices_sensor = np.random.choice(df.index, size=int(n_rows * 0.15), replace=False)\n",
    "df.loc[mcar_indices_sensor, 'sensor_reading'] = np.nan\n",
    "\n",
    "# --- 3. Categorical Column with MCAR: `preferred_device` ---\n",
    "df['preferred_device'] = np.random.choice(['Phone', 'Tablet', 'Laptop', 'Desktop'], size=n_rows, p=[0.4, 0.2, 0.3, 0.1])\n",
    "# Introduce MCAR missingness (approx 10%)\n",
    "mcar_indices_device = np.random.choice(df.index, size=int(n_rows * 0.10), replace=False)\n",
    "df.loc[mcar_indices_device, 'preferred_device'] = np.nan\n",
    "\n",
    "# --- 4. Numerical Column with NMAR: `reported_income` ---\n",
    "# True income (hypothetical, not directly in final dataset in this form)\n",
    "# Let's make true income somewhat related to age for realism, but the NMAR part is key\n",
    "true_income = 20000 + (df['age'] * 500) + np.random.normal(0, 15000, n_rows)\n",
    "true_income[true_income < 10000] = 10000 # Floor income\n",
    "df['reported_income'] = true_income.round(0)\n",
    "\n",
    "# NMAR mechanism: People with very high or very low incomes are less likely to report\n",
    "# Define thresholds based on the 'true_income' distribution\n",
    "low_income_thresh = np.percentile(true_income, 15)  # Bottom 15%\n",
    "high_income_thresh = np.percentile(true_income, 85) # Top 15%\n",
    "\n",
    "# Probabilities of missing\n",
    "prob_missing_low_income = 0.70  # 70% chance of missing if income is low\n",
    "prob_missing_high_income = 0.60 # 60% chance of missing if income is high\n",
    "prob_missing_mid_income = 0.05  # 5% chance for others\n",
    "\n",
    "for i in df.index:\n",
    "    # Use the 'true_income' for the decision, then make 'reported_income' missing\n",
    "    actual_val = true_income[i] # The value that would have been reported\n",
    "    if actual_val < low_income_thresh:\n",
    "        if np.random.rand() < prob_missing_low_income:\n",
    "            df.loc[i, 'reported_income'] = np.nan\n",
    "    elif actual_val > high_income_thresh:\n",
    "        if np.random.rand() < prob_missing_high_income:\n",
    "            df.loc[i, 'reported_income'] = np.nan\n",
    "    else:\n",
    "        if np.random.rand() < prob_missing_mid_income:\n",
    "            df.loc[i, 'reported_income'] = np.nan\n",
    "\n",
    "\n",
    "# --- 5. Categorical Column with NMAR: `satisfaction_level` ---\n",
    "# True satisfaction (hypothetical)\n",
    "true_satisfaction = np.random.choice(['Very Unsatisfied', 'Unsatisfied', 'Neutral', 'Satisfied', 'Very Satisfied'],\n",
    "                                     size=n_rows, p=[0.1, 0.15, 0.3, 0.3, 0.15])\n",
    "df['satisfaction_level'] = true_satisfaction\n",
    "\n",
    "# NMAR mechanism: People who are 'Very Unsatisfied' are less likely to report their satisfaction\n",
    "prob_missing_very_unsatisfied = 0.75 # 75% chance of missing if 'Very Unsatisfied'\n",
    "prob_missing_other_satisfaction = 0.05 # 5% for other categories\n",
    "\n",
    "for i in df.index:\n",
    "    actual_val = true_satisfaction[i] # The value that would have been reported\n",
    "    if actual_val == 'Very Unsatisfied':\n",
    "        if np.random.rand() < prob_missing_very_unsatisfied:\n",
    "            df.loc[i, 'satisfaction_level'] = np.nan\n",
    "    else:\n",
    "        # Small chance of missing for other categories too, to make it less obvious\n",
    "        if np.random.rand() < prob_missing_other_satisfaction:\n",
    "             df.loc[i, 'satisfaction_level'] = np.nan\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"Dataset head:\")\n",
    "print(df.head())\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n--- Verifying NMAR ---\")\n",
    "# For reported_income (Numerical NMAR)\n",
    "# We expect more NaNs where true_income was low or high\n",
    "# This is a bit tricky to verify directly without the 'true_income' column,\n",
    "# but we designed it that way. We can look at the distribution of non-missing values.\n",
    "if 'reported_income' in df.columns and df['reported_income'].isnull().any():\n",
    "    print(\"\\nReported Income (Numerical NMAR):\")\n",
    "    print(\"Distribution of non-missing reported_income:\")\n",
    "    print(df['reported_income'].dropna().describe())\n",
    "    # If NMAR worked, the tails of the original distribution should be underrepresented.\n",
    "\n",
    "# For satisfaction_level (Categorical NMAR)\n",
    "# We expect 'Very Unsatisfied' to be underrepresented in non-missing values,\n",
    "# and a higher proportion of its original occurrences to be NaN.\n",
    "if 'satisfaction_level' in df.columns and df['satisfaction_level'].isnull().any():\n",
    "    print(\"\\nSatisfaction Level (Categorical NMAR):\")\n",
    "    print(\"Value counts of non-missing satisfaction_level:\")\n",
    "    print(df['satisfaction_level'].dropna().value_counts(normalize=True))\n",
    "    print(\"\\nOriginal (true) distribution of satisfaction_level (for comparison):\")\n",
    "    print(pd.Series(true_satisfaction).value_counts(normalize=True))\n",
    "    # We expect 'Very Unsatisfied' to have a lower proportion in the non-missing data\n",
    "    # than in the original 'true_satisfaction' data.\n",
    "\n",
    "# Check that not all columns have missing values\n",
    "if df.isnull().all().any():\n",
    "    print(\"\\nERROR: At least one column has ALL values missing, which shouldn't happen.\")\n",
    "elif not df.isnull().any().all():\n",
    "    print(\"\\nSUCCESS: Not all columns have missing values.\")\n",
    "else:\n",
    "    print(\"\\nERROR: All columns have at least one missing value.\")\n",
    "\n",
    "\n",
    "# Check number of columns with missing values\n",
    "cols_with_missing = df.isnull().any()\n",
    "num_cols_with_missing = cols_with_missing.sum()\n",
    "print(f\"\\nNumber of columns with missing values: {num_cols_with_missing}\")\n",
    "if num_cols_with_missing >= 4:\n",
    "    print(\"SUCCESS: At least 4 columns have missing values.\")\n",
    "else:\n",
    "    print(f\"FAILURE: Expected at least 4 columns with missing values, got {num_cols_with_missing}.\")\n",
    "\n",
    "# Verify specific column types and missingness patterns\n",
    "print(\"\\nSummary of designed missingness:\")\n",
    "print(\"- sensor_reading (Numerical MCAR): Missingness should be random.\")\n",
    "print(\"- preferred_device (Categorical MCAR): Missingness should be random.\")\n",
    "print(\"- reported_income (Numerical NMAR): Missingness depends on its own unobserved value (higher for very low/high incomes).\")\n",
    "print(\"- satisfaction_level (Categorical NMAR): Missingness depends on its own unobserved value (higher for 'Very Unsatisfied').\")\n",
    "\n",
    "# To save to CSV:\n",
    "# df.to_csv(\"dataset_with_missing_values.csv\", index=False)\n",
    "# print(\"\\nDataset saved to dataset_with_missing_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5440782b-41c3-49e9-8ecf-ff2917ca0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ba152464-5b84-4c6f-a86d-f191770bfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0a7571ce-b69e-4fcc-9053-6967813e31bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>has_subscription</th>\n",
       "      <th>sensor_reading</th>\n",
       "      <th>preferred_device</th>\n",
       "      <th>reported_income</th>\n",
       "      <th>satisfaction_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>True</td>\n",
       "      <td>110.46</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>50307.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>Paris</td>\n",
       "      <td>False</td>\n",
       "      <td>88.53</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unsatisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>Paris</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>17233.0</td>\n",
       "      <td>Very Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>Paris</td>\n",
       "      <td>True</td>\n",
       "      <td>142.85</td>\n",
       "      <td>Phone</td>\n",
       "      <td>40273.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>London</td>\n",
       "      <td>False</td>\n",
       "      <td>134.55</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age    city  has_subscription  sensor_reading preferred_device  \\\n",
       "0   1   56  Berlin              True          110.46           Laptop   \n",
       "1   2   69   Paris             False           88.53           Tablet   \n",
       "2   3   46   Paris              True             NaN           Tablet   \n",
       "3   4   32   Paris              True          142.85            Phone   \n",
       "4   5   60  London             False          134.55          Desktop   \n",
       "\n",
       "   reported_income satisfaction_level  \n",
       "0          50307.0            Neutral  \n",
       "1              NaN        Unsatisfied  \n",
       "2          17233.0     Very Satisfied  \n",
       "3          40273.0            Neutral  \n",
       "4              NaN            Neutral  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9126eec5-2931-4c7d-9805-5cd1363cc795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0.0\n",
       "age                    0.0\n",
       "city                   0.0\n",
       "has_subscription       0.0\n",
       "sensor_reading        15.0\n",
       "preferred_device      10.0\n",
       "reported_income       22.4\n",
       "satisfaction_level    11.4\n",
       "dtype: float64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "612fbb9e-d06b-41fb-b720-730051280709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction_level\n",
       "Satisfied           148\n",
       "Neutral             138\n",
       "Unsatisfied          76\n",
       "Very Satisfied       68\n",
       "Very Unsatisfied     13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handling missing vals in satisfaction level\n",
    "\n",
    "df['satisfaction_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "158f30d2-51b5-40a6-bf27-b6250253cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  500 non-null    int64  \n",
      " 1   age                 500 non-null    int32  \n",
      " 2   city                500 non-null    object \n",
      " 3   has_subscription    500 non-null    bool   \n",
      " 4   sensor_reading      425 non-null    float64\n",
      " 5   preferred_device    450 non-null    object \n",
      " 6   reported_income     388 non-null    float64\n",
      " 7   satisfaction_level  443 non-null    object \n",
      "dtypes: bool(1), float64(2), int32(1), int64(1), object(3)\n",
      "memory usage: 26.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8e51055b-ddf6-43fb-a156-80c66643085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3185ba0f-8ceb-45d7-bb52-b6903b721aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the missing vals from the target\n",
    "\n",
    "ddf.dropna(subset=['satisfaction_level'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9fc454b7-9661-4698-b58e-f1a3b0ce39e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 443 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  443 non-null    int64  \n",
      " 1   age                 443 non-null    int32  \n",
      " 2   city                443 non-null    object \n",
      " 3   has_subscription    443 non-null    bool   \n",
      " 4   sensor_reading      380 non-null    float64\n",
      " 5   preferred_device    401 non-null    object \n",
      " 6   reported_income     339 non-null    float64\n",
      " 7   satisfaction_level  443 non-null    object \n",
      "dtypes: bool(1), float64(2), int32(1), int64(1), object(3)\n",
      "memory usage: 26.4+ KB\n"
     ]
    }
   ],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "61628552-5d7b-4a5a-b6fd-34524976fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_subscription'] =df['has_subscription'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9b4c6f47-57cc-4584-9472-ff2814112698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x = ddf.iloc[:, 0:-1]\n",
    "y = ddf.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b36e937d-20ac-4925-8ba8-9aae31834638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0.000000\n",
       "age                  0.000000\n",
       "city                 0.000000\n",
       "has_subscription     0.000000\n",
       "sensor_reading      14.361702\n",
       "preferred_device     9.042553\n",
       "reported_income     23.138298\n",
       "dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "44970b65-d6a7-4a8a-9be5-1b3b90b11676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preferred_device\n",
       "Phone      139\n",
       "Laptop      97\n",
       "Tablet      71\n",
       "Desktop     35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['preferred_device'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850be861-3768-441f-8032-a0958efc636b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6e065854-6cf3-4775-bf4b-411d65ea0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the x cols\n",
    "\n",
    "tnf1 = ColumnTransformer([\n",
    "    ('tnf1', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'), ['preferred_device', 'city']),\n",
    "    ('tnf3', OrdinalEncoder(categories=[[False, True]]), ['has_subscription'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "da97a0bc-f404-4ad8-9767-d8157f389cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivar imputation \n",
    "\n",
    "tnf2 = ColumnTransformer([\n",
    "    ('tnf2', IterativeImputer(n_nearest_features=4, max_iter=50, random_state=42), slice(1, 13))\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d87f9771-08cd-4424-84c2-a6b187d3f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN imputer (backup)\n",
    "\n",
    "tnf4 = ColumnTransformer([\n",
    "    ('tnf2', KNNImputer(n_neighbors=4, weights='uniform'), slice(1, 13))\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3a682cdd-daee-4e90-ad2a-0cc123c334f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "\n",
    "tnf4 = ColumnTransformer([\n",
    "    ('scale', StandardScaler(), slice(0, 13))\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ebd013e7-8d19-418b-a247-508c5ea50a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to streamline\n",
    "\n",
    "pipe1 = Pipeline(steps=[\n",
    "    ('tnf1', tnf1),\n",
    "    ('tnf2', tnf2),\n",
    "    ('tnf3', tnf3)\n",
    "])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline(steps=[\n",
    "    ('tnf1', tnf1),\n",
    "    ('tnf4', tnf4),\n",
    "    ('tnf3', tnf3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "eb8eb2e9-2e0a-4db2-924a-56c7e4d6bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfd_train = pipe1.fit_transform(x_train)\n",
    "trfd_test = pipe1.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c8d9fb17-2387-4d1b-a799-60536ddfbac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4167382f-233c-417e-a3d1-508403ce3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfd_ytrain = label.fit_transform(y_train)\n",
    "trfd_ytest = label.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f2ba8-5e38-43a2-834d-7417761c8867",
   "metadata": {},
   "source": [
    "## Iterartive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3130d75f-bd09-4415-ae38-3d77bdec8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31343283582089554\n",
      "0.3283582089552239\n",
      "0.3283582089552239\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = RandomForestClassifier()\n",
    "\n",
    "\n",
    "clf.fit(trfd_train, trfd_ytrain)\n",
    "clf2.fit(trfd_train, trfd_ytrain)\n",
    "clf3.fit(trfd_train, trfd_ytrain)\n",
    "\n",
    "pred = clf.predict(trfd_test)\n",
    "pred2 = clf2.predict(trfd_test)\n",
    "pred3 = clf2.predict(trfd_test)\n",
    "\n",
    "print(accuracy_score(trfd_ytest, pred))\n",
    "print(accuracy_score(trfd_ytest, pred2))\n",
    "print(accuracy_score(trfd_ytest, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fcd3b1b2-db1f-42c1-b1da-cb9ee1c0d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfd_train = pipe2.fit_transform(x_train)\n",
    "trfd_test = pipe2.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ec765-b2a3-4cee-8bc2-427ef97bec88",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "11dbccfd-efe7-4553-b063-0e4bbd735dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[176]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m clf3 = RandomForestClassifier()\n\u001b[32m      4\u001b[39m clf4 = GradientBoostingClassifier()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrfd_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrfd_ytrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m clf2.fit(trfd_train, trfd_ytrain)\n\u001b[32m      8\u001b[39m clf3.fit(trfd_train, trfd_ytrain)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mlc\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = RandomForestClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "\n",
    "clf.fit(trfd_train, trfd_ytrain)\n",
    "clf2.fit(trfd_train, trfd_ytrain)\n",
    "clf3.fit(trfd_train, trfd_ytrain)\n",
    "clf4.fit(trfd_train, trfd_ytrain)\n",
    "\n",
    "pred = clf.predict(trfd_test)\n",
    "pred2 = clf2.predict(trfd_test)\n",
    "pred3 = clf2.predict(trfd_test)\n",
    "pred4 = clf2.predict(trfd_test)\n",
    "\n",
    "print(accuracy_score(trfd_ytest, pred))\n",
    "print(accuracy_score(trfd_ytest, pred2))\n",
    "print(accuracy_score(trfd_ytest, pred3))\n",
    "print(accuracy_score(trfd_ytest, pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb0224-c630-4929-b16e-6afc3ac39652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['satisfaction_level'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d254d9-7701-4441-8ab6-0751d5316321",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fabef8-908d-4b36-a9a8-9e0226cd5a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
