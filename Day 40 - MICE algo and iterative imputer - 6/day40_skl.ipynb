{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5fd5a2-19b8-4b77-888c-3c0a84577e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified DataFrame head:\n",
      "   R&D Spend  Administration  Marketing Spend       State\n",
      "0  165349.20       136897.80        471784.10    New York\n",
      "1  162597.70       151377.59        443898.53  California\n",
      "2  153441.51       101145.55        407934.54     Florida\n",
      "3  144372.41       118671.85        383199.62    New York\n",
      "4  142107.34        91391.77        366168.42     Florida\n",
      "\n",
      "Modified DataFrame info (to see NaN counts):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   R&D Spend        46 non-null     float64\n",
      " 1   Administration   49 non-null     float64\n",
      " 2   Marketing Spend  50 non-null     float64\n",
      " 3   State            46 non-null     object \n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 1.7+ KB\n",
      "\n",
      "Total NaNs introduced in the DataFrame: 9\n",
      "Percentage of NaNs in the (selected columns of the) DataFrame: 4.50%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import random\n",
    "\n",
    "def create_dataframe_with_missing_values(csv_data_string: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a CSV string, removes the 'Profit' column, and adds random missing values \n",
    "    (NaN) to the remaining four specified features. The number of NaNs introduced \n",
    "    will be random, greater than zero (if permissible), and strictly less than 5% \n",
    "    of the total cells in these four features.\n",
    "    Returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Parse CSV input string into a pandas DataFrame\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(csv_data_string))\n",
    "    except pd.errors.EmptyDataError:\n",
    "        # If the input string is empty or results in an empty DataFrame initially\n",
    "        return pd.DataFrame() \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing CSV string: {e}\")\n",
    "\n",
    "    # 2. Define the features for NaN introduction and remove 'Profit'\n",
    "    features_for_nan_introduction = ['R&D Spend', 'Administration', 'Marketing Spend', 'State']\n",
    "    profit_column = 'Profit'\n",
    "    \n",
    "    # Check if profit column exists before trying to drop it\n",
    "    if profit_column in df.columns:\n",
    "        df_processed = df.drop(columns=[profit_column])\n",
    "    else:\n",
    "        # If 'Profit' column is already missing, work with the current df\n",
    "        # but ensure it only contains the expected features for NaN introduction if possible\n",
    "        df_processed = df.copy() \n",
    "\n",
    "    # Ensure df_processed contains only the target features if other columns were present\n",
    "    # or if 'Profit' was already missing.\n",
    "    # This also reorders columns to match features_for_nan_introduction if they exist.\n",
    "    # If some of these features are missing from the input, this will raise a KeyError.\n",
    "    # We should be robust to cases where the input might not perfectly match.\n",
    "    \n",
    "    # Filter to only include the features_for_nan_introduction that are actually in df_processed\n",
    "    existing_features_for_nan = [col for col in features_for_nan_introduction if col in df_processed.columns]\n",
    "    if not existing_features_for_nan:\n",
    "        # If none of the target columns for NaN introduction exist, return the (potentially modified) df\n",
    "        return df_processed \n",
    "        \n",
    "    df_processed = df_processed[existing_features_for_nan]\n",
    "\n",
    "    # 3. Calculate the number of NaNs to introduce\n",
    "    num_rows = df_processed.shape[0]\n",
    "    \n",
    "    if num_rows == 0: # Handle empty DataFrame after processing\n",
    "        return df_processed\n",
    "\n",
    "    num_feature_cols = len(existing_features_for_nan) # Number of columns we'll actually modify\n",
    "    total_cells_in_features = num_rows * num_feature_cols\n",
    "\n",
    "    if total_cells_in_features == 0: # No cells to modify\n",
    "        return df_processed\n",
    "\n",
    "    # Constraint: number of NaNs (k) must be k < 0.05 * total_cells_in_features\n",
    "    strict_upper_limit_float = total_cells_in_features * 0.05\n",
    "\n",
    "    # Determine the maximum integer k (max_permissible_nans) that satisfies k < strict_upper_limit_float\n",
    "    if strict_upper_limit_float <= 0:\n",
    "        max_permissible_nans = 0\n",
    "    else:\n",
    "        # If strict_upper_limit_float is an integer (e.g., 10.0), k_max must be strict_upper_limit_float - 1\n",
    "        # If strict_upper_limit_float is float (e.g., 9.8), k_max is floor(9.8) = 9\n",
    "        if strict_upper_limit_float == np.floor(strict_upper_limit_float): \n",
    "            max_permissible_nans = int(strict_upper_limit_float) - 1\n",
    "        else: \n",
    "            max_permissible_nans = int(np.floor(strict_upper_limit_float))\n",
    "    \n",
    "    max_permissible_nans = max(0, max_permissible_nans) # Ensure it's not negative\n",
    "\n",
    "    # Determine the actual number of NaNs to add\n",
    "    num_nans_to_add = 0\n",
    "    if max_permissible_nans > 0:\n",
    "        # \"add missing values\" implies at least one if possible.\n",
    "        # \"random\" can apply to the quantity.\n",
    "        num_nans_to_add = random.randint(1, max_permissible_nans)\n",
    "    \n",
    "    # 4. Introduce NaNs into the DataFrame (df_processed)\n",
    "    if num_nans_to_add > 0:\n",
    "        candidate_cells = []\n",
    "        for r_idx in range(num_rows):\n",
    "            for col_name in existing_features_for_nan: # Iterate through the columns of df_processed\n",
    "                candidate_cells.append((r_idx, col_name))\n",
    "        \n",
    "        # Ensure we don't try to sample more NaNs than available cells\n",
    "        # (although num_nans_to_add should already be less than total_cells_in_features)\n",
    "        if num_nans_to_add > len(candidate_cells):\n",
    "            num_nans_to_add = len(candidate_cells)\n",
    "\n",
    "        if num_nans_to_add > 0 : # Re-check, as len(candidate_cells) could be 0 for very small inputs\n",
    "            cells_for_nan = random.sample(candidate_cells, k=num_nans_to_add)\n",
    "            \n",
    "            for r_idx, col_name in cells_for_nan:\n",
    "                df_processed.loc[r_idx, col_name] = np.nan\n",
    "            \n",
    "    # 5. Return the final pandas DataFrame\n",
    "    return df_processed\n",
    "\n",
    "# Provided CSV data string\n",
    "csv_data = \"\"\"R&D Spend,Administration,Marketing Spend,State,Profit\n",
    "165349.2,136897.8,471784.1,New York,192261.83\n",
    "162597.7,151377.59,443898.53,California,191792.06\n",
    "153441.51,101145.55,407934.54,Florida,191050.39\n",
    "144372.41,118671.85,383199.62,New York,182901.99\n",
    "142107.34,91391.77,366168.42,Florida,166187.94\n",
    "131876.9,99814.71,362861.36,New York,156991.12\n",
    "134615.46,147198.87,127716.82,California,156122.51\n",
    "130298.13,145530.06,323876.68,Florida,155752.6\n",
    "120542.52,148718.95,311613.29,New York,152211.77\n",
    "123334.88,108679.17,304981.62,California,149759.96\n",
    "101913.08,110594.11,229160.95,Florida,146121.95\n",
    "100671.96,91790.61,249744.55,California,144259.4\n",
    "93863.75,127320.38,249839.44,Florida,141585.52\n",
    "91992.39,135495.07,252664.93,California,134307.35\n",
    "119943.24,156547.42,256512.92,Florida,132602.65\n",
    "114523.61,122616.84,261776.23,New York,129917.04\n",
    "78013.11,121597.55,264346.06,California,126992.93\n",
    "94657.16,145077.58,282574.31,New York,125370.37\n",
    "91749.16,114175.79,294919.57,Florida,124266.9\n",
    "86419.7,153514.11,0,New York,122776.86\n",
    "76253.86,113867.3,298664.47,California,118474.03\n",
    "78389.47,153773.43,299737.29,New York,111313.02\n",
    "73994.56,122782.75,303319.26,Florida,110352.25\n",
    "67532.53,105751.03,304768.73,Florida,108733.99\n",
    "77044.01,99281.34,140574.81,New York,108552.04\n",
    "64664.71,139553.16,137962.62,California,107404.34\n",
    "75328.87,144135.98,134050.07,Florida,105733.54\n",
    "72107.6,127864.55,353183.81,New York,105008.31\n",
    "66051.52,182645.56,118148.2,Florida,103282.38\n",
    "65605.48,153032.06,107138.38,New York,101004.64\n",
    "61994.48,115641.28,91131.24,Florida,99937.59\n",
    "61136.38,152701.92,88218.23,New York,97483.56\n",
    "63408.86,129219.61,46085.25,California,97427.84\n",
    "55493.95,103057.49,214634.81,Florida,96778.92\n",
    "46426.07,157693.92,210797.67,California,96712.8\n",
    "46014.02,85047.44,205517.64,New York,96479.51\n",
    "28663.76,127056.21,201126.82,Florida,90708.19\n",
    "44069.95,51283.14,197029.42,California,89949.14\n",
    "20229.59,65947.93,185265.1,New York,81229.06\n",
    "38558.51,82982.09,174999.3,California,81005.76\n",
    "28754.33,118546.05,172795.67,California,78239.91\n",
    "27892.92,84710.77,164470.71,Florida,77798.83\n",
    "23640.93,96189.63,148001.11,California,71498.49\n",
    "15505.73,127382.3,35534.17,New York,69758.98\n",
    "22177.74,154806.14,28334.72,California,65200.33\n",
    "1000.23,124153.04,1903.93,New York,64926.08\n",
    "1315.46,115816.21,297114.46,Florida,49490.75\n",
    "0,135426.92,0,California,42559.73\n",
    "542.05,51743.15,0,New York,35673.41\n",
    "0,116983.8,45173.06,California,14681.4\n",
    "\"\"\"\n",
    "\n",
    "# Generate the modified DataFrame\n",
    "modified_df = create_dataframe_with_missing_values(csv_data)\n",
    "\n",
    "# Display the DataFrame (or parts of it)\n",
    "print(\"Modified DataFrame head:\")\n",
    "print(modified_df.head())\n",
    "print(\"\\nModified DataFrame info (to see NaN counts):\")\n",
    "modified_df.info()\n",
    "\n",
    "# You can also check the total number of NaNs\n",
    "total_nans_in_df = modified_df.isnull().sum().sum()\n",
    "print(f\"\\nTotal NaNs introduced in the DataFrame: {total_nans_in_df}\")\n",
    "\n",
    "# Verify the percentage of NaNs\n",
    "num_rows_final = modified_df.shape[0]\n",
    "num_cols_final = modified_df.shape[1] # This will be 4 if all target columns were present\n",
    "total_cells_final = num_rows_final * num_cols_final\n",
    "if total_cells_final > 0:\n",
    "    percentage_nans = (total_nans_in_df / total_cells_final) * 100\n",
    "    print(f\"Percentage of NaNs in the (selected columns of the) DataFrame: {percentage_nans:.2f}%\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, no NaNs to calculate percentage for.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69ef023-c3ae-49c2-a532-f0daca40606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c39c0c-a991-46b7-abf2-797080eb79cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   R&D Spend        46 non-null     float64\n",
      " 1   Administration   49 non-null     float64\n",
      " 2   Marketing Spend  50 non-null     float64\n",
      " 3   State            46 non-null     object \n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "modified_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bfd96b0-e603-43d9-add0-95ee19e51b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('50_Startups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6d3d13-e3e6-4486-a973-7fd0d1ed8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_df['Profits'] = df['Profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95b2111-2022-4769-8cd6-a4b8f0c0ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a01081-e584-4b0a-9510-d61dc586b124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R&D Spend          8.0\n",
       "Administration     2.0\n",
       "Marketing Spend    0.0\n",
       "State              8.0\n",
       "Profits            0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d15fa94-2147-4786-bc31-0b0956a7537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = ddf.iloc[:, 0:-1]\n",
    "y = ddf.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0412dfa6-bbc9-4981-8fa6-bfe5b33f2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1636fb85-9895-4e8a-b652-245182c21fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tnf2 = ColumnTransformer([\n",
    "    ('trf1', StandardScaler(), slice(0, 5))\n",
    "])\n",
    "\n",
    "tnf3 = ColumnTransformer([\n",
    "    ('trf1', IterativeImputer(), slice(0, 5))\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipes = Pipeline(steps=[\n",
    "    ('tnf1', tnf1),\n",
    "    ('tnf2', tnf2),\n",
    "    ('tnf3', tnf3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57401c72-9f68-4773-b4f7-7aea2adcad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfd_train = pipes.fit_transform(x_train)\n",
    "trfd_test = pipes.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3544f60-8543-470a-9f40-1ca9d5b4eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916828246835651\n",
      "0.8501813438676471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf2 = DecisionTreeRegressor()\n",
    "\n",
    "clf.fit(trfd_train, y_train)\n",
    "clf2.fit(trfd_train, y_train)\n",
    "\n",
    "pred = clf.predict(trfd_test)\n",
    "pred2 = clf2.predict(trfd_test)\n",
    "print(r2_score(y_test, pred))\n",
    "print(r2_score(y_test, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30f0784c-1673-4ee9-b435-790a4f9c9322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9182315666537786\n",
      "0.8737027955370668\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cross_val_score(estimator=clf, X=trfd_train, y=y_train, scoring='r2')))\n",
    "print(np.mean(cross_val_score(estimator=clf2, X=trfd_train, y=y_train, scoring='r2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3cd14-885c-4de2-a6b5-06139fbc5561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
